# -*- coding: utf-8 -*-
"""pdf_chat_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N1rNLVh_5OTh4cKhSuMox4TIPQTxGz2Q
"""

!pip install -qU langchain

!pip install chromadb

!pip install pypdf

!pip install openai

!pip install -qU huggingface_hub

!python -m venv langchainenv

!source langchainenv/bin/activate

!git clone https://github.com/nicknochnack/LangchainDocuments

!git clone https://github.com/youssefHosni/Chat-with-Pdf

!cd LangchainDocuments

!pip install -r /content/Chat-with-Pdf/requirements.txt

!pip install streamlit

!pip install pyngrok

!pip install tiktoken

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import tempfile
# from PIL import Image
# 
# # Import os to set API key
# import os
# # Import OpenAI as main LLM service
# from langchain.llms import OpenAI
# from langchain.embeddings import OpenAIEmbeddings
# # Bring in streamlit for UI/app interface
# import streamlit as st
# 
# # Import PDF document loaders...there's other ones as well!
# from langchain.document_loaders import PyPDFLoader
# # Import chroma as the vector store
# from langchain.vectorstores import Chroma
# 
# # Import vector store stuff
# from langchain.agents.agent_toolkits import (
#     create_vectorstore_agent,
#     VectorStoreToolkit,
#     VectorStoreInfo
# )
# 
# 
# # Set the title and subtitle of the app
# st.title('ðŸ”ŽðŸ“„ Paper Chatter : Interact with Your Research Paper in a Conversational Way!')
# st.subheader('Load your Paper, ask questions, and receive answers directly from the document.')
# 
# # Load the image
# image = Image.open('/content/Chat-with-Pdf/logo.png')
# st.image(image)
# 
# # Loading the Pdf file and return a temporary path for it
# st.subheader('Upload your pdf')
# uploaded_file = st.file_uploader('', type=(['pdf',"tsv","csv","txt","tab","xlsx","xls"]))
# 
# temp_file_path = os.getcwd()
# while uploaded_file is None:
#     x = 1
# 
# if uploaded_file is not None:
#     # Save the uploaded file to a temporary location
#     temp_dir = tempfile.TemporaryDirectory()
#     temp_file_path = os.path.join(temp_dir.name, uploaded_file.name)
#     with open(temp_file_path, "wb") as temp_file:
#         temp_file.write(uploaded_file.read())
# 
#     st.write("Full path of the uploaded file:", temp_file_path)
# 
# # Set APIkey for OpenAI Service
# # Can sub this out for other LLM providers
# os.environ['OPENAI_API_KEY'] ='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'
# 
# # Create instance of OpenAI LLM
# llm = OpenAI(temperature=0.1, verbose=True)
# embeddings = OpenAIEmbeddings()
# 
# # Create and load PDF Loader
# loader = PyPDFLoader(temp_file_path)
# # Split pages from pdf
# pages = loader.load_and_split()
# 
# # Load documents into vector database aka ChromaDB
# store = Chroma.from_documents(pages, embeddings, collection_name='Pdf')
# 
# # Create vectorstore info object
# vectorstore_info = VectorStoreInfo(
#     name="Pdf",
#     description=" A pdf file to answer your questions",
#     vectorstore=store
# )
# # Convert the document store into a langchain toolkit
# toolkit = VectorStoreToolkit(vectorstore_info=vectorstore_info)
# 
# # Add the toolkit to an end-to-end LC
# agent_executor = create_vectorstore_agent(
#     llm=llm,
#     toolkit=toolkit,
#     verbose=True
# )
# 
# # Create a text input box for the user
# prompt = st.text_input('Input your prompt here')
# 
# # If the user hits enter
# if prompt:
#     # Then pass the prompt to the LLM
#     response = agent_executor.run(prompt)
#     # ...and write it out to the screen
#     st.write(response)
# 
#     # With a streamlit expander
#     with st.expander('Document Similarity Search'):
#         # Find the relevant pages
#         search = store.similarity_search_with_score(prompt)
#         # Write out the first
#         st.write(search[0][0].page_content)

!streamlit run app.py &>/dev/null&

!ngrok authtoken 2KLDcEYQffLtLRYFoNmY5pYdXJ9_79A5WjqVM3aoviv6xrmUP

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip

!unzip ngrok-stable-linux-amd64.zip

get_ipython().system_raw('./ngrok http 8501 &')

! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

!streamlit run /content/app.py

